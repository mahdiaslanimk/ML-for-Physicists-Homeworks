# Homework for Lecture 1

## Problem 1ðŸŒŸ:

Implement a network that computes XOR (arbitrary number of hidden layers); meaning: the output should be +1 for y1 y2<0 and 0 otherwise!



## Problem 2:

Implement a network that approximately or exactly computes XOR, with just 1 hidden layer(!)



## Problem 3ðŸŒŸ:

Visualize the results of intermediate layers in a multi-layer randomly initialized NN (meaning: take a fixed randomly initialized multi-layer network, and then throw away the layers above layer n; and directly connect layer n to the output layer; see how results change when you vary n; you can start from the notebook [01_MachineLearning_Basics_NeuralNetworksPython.ipynb](https://owncloud.gwdg.de/index.php/s/Unl2Yru1HsqwQNK)



## Problem 4:

What happens when you change the spread of the random weights? Smart weight initialization is an important point for NN training.



## Problem 5:

Explore cases of curve fitting where there are several (non-equivalent) local minima. Is sampling noise helpful (i.e. the noise that comes about because of the small number of x samples)?



---

<sup>ðŸŒŸ</sup>: Suggested by [Dr. Florian Marquardt](https://scholar.google.com/citations?user=jx_c7SgAAAAJ&hl=en&oi=ao) (Essential)

