{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-corporation",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "## Problem 3*:\n",
    "\n",
    "Visualize the results of intermediate layers in a multi-layer randomly initialized NN (meaning: take a fixed randomly initialized multi-layer network, and then throw away the layers above layer n; and directly connect layer n to the output layer; see how results change when you vary n; you can start from the notebook [01_MachineLearning_Basics_NeuralNetworksPython.ipynb](https://owncloud.gwdg.de/index.php/s/Unl2Yru1HsqwQNK)\n",
    "\n",
    "\n",
    "\n",
    "## Problem 4:\n",
    "\n",
    "What happens when you change the spread of the random weights? Smart weight initialization is an important point for NN training.\n",
    "\n",
    "<strong style=\"color: rgb(52,199,89)\">Solution:</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "import seaborn as sns\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "# making giff images\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 2          # number of neurons of input layer\n",
    "Nh_max = 30     # number of hidden layers\n",
    "LayerSize = 30  # number of neurons of each hidden layer\n",
    "N1 =1           # number of neurons of output layer\n",
    "\n",
    "np.random.seed(seed=100)\n",
    "\n",
    "################### Uniform Distribution ###################\n",
    "high_w = 30\n",
    "high_b = 10\n",
    "# weights and biases of hidden layers (uniform dist.)\n",
    "Weights = np.random.uniform(low=-high_w, high=high_w, size=[Nh_max,LayerSize,LayerSize])\n",
    "Biases  = np.random.uniform(low=-high_b, high=high_b, size=[Nh_max,LayerSize])\n",
    "\n",
    "# weights and biases for the first hidden layer (coming in from input layer)\n",
    "WeightsFirst = np.random.uniform(low=-high_w, high=high_w, size=[N0,LayerSize])\n",
    "BiasesFirst  = np.random.uniform(low=-high_b, high=high_b, size=LayerSize)\n",
    "\n",
    "# weights and biases for the final layer (i.e. the output neuron)\n",
    "WeightsFinal = np.random.uniform(low=-high_b, high=high_b, size=[LayerSize,N1])\n",
    "BiasesFinal  = np.random.uniform(low=-high_b, high=high_b, size=N1)\n",
    "################### Uniform Distribution ###################\n",
    "\n",
    "\n",
    "############## Noraml (Gaussian) Distribution ##############\n",
    "# scale_w = 10\n",
    "# scale_b = 3\n",
    "# # weights and biases of hidden layers(normal(gaussian dist))\n",
    "# Weights = np.random.normal(scale = scale_w , size = [Nh_max,LayerSize,LayerSize])\n",
    "# Biases  = np.random.normal(scale = scale_b , size = [Nh_max,LayerSize])\n",
    "\n",
    "# # weights and biases for the first hidden layer (coming in from input layer)\n",
    "# WeightsFirst = np.random.normal(scale = scale_w, size = [N0,LayerSize])\n",
    "# BiasesFirst  = np.random.normal(scale = scale_b, size = LayerSize)\n",
    "\n",
    "# # weights and biases for the final layer (i.e. the output neuron)\n",
    "# WeightsFinal = np.random.normal(scale = scale_b, size = [LayerSize,N1])\n",
    "# BiasesFinal  = np.random.normal(scale = scale_b, size = N1)\n",
    "############## Noraml (Gaussian) Distribution ##############\n",
    "\n",
    "\n",
    "def apply_layer_new(y_in,w,b):    # a function that applies a layer \n",
    "    z=np.dot(y_in,w)+b            # note different order in matrix product!\n",
    "    return(1/(1+np.exp(-z)))\n",
    "\n",
    "def apply_multi_net(y_in,mid_Weights, mid_Biases,Nh):\n",
    "    global mid_WeightsFinal, mid_BiasesFinal\n",
    "    \n",
    "    y=apply_layer_new(y_in,WeightsFirst,BiasesFirst)    \n",
    "    for j in range(Nh):\n",
    "        y=apply_layer_new(y,mid_Weights[j,:,:],mid_Biases[j,:])      \n",
    "    output=apply_layer_new(y,WeightsFinal,BiasesFinal)\n",
    "    return(output)\n",
    "\n",
    "M = 200\n",
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "v0,v1 = np.meshgrid(np.linspace(-0.5,0.5,M),np.linspace(-0.5,0.5,M))\n",
    "batchsize = M**2 # number of samples = number of pixels = M^2\n",
    "y_in = np.zeros([batchsize,2])\n",
    "y_in[:,0] = v0.flatten() # fill first component (index 0)\n",
    "y_in[:,1] = v1.flatten() # fill second component\n",
    "\n",
    "# use the MxM input grid that we generated above \n",
    "y_out_list = []\n",
    "for i in range(Nh_max):\n",
    "    mid_Weights = Weights[:i]\n",
    "    mid_Biases  = Biases[:i]\n",
    "    y_out=apply_multi_net(y_in, mid_Weights, mid_Biases, i) # apply net to all these samples!\n",
    "    y_out_list.append(y_out)\n",
    "    \n",
    "y_2D_list = []\n",
    "for y_out in y_out_list:\n",
    "    y_2D = np.reshape(y_out[:,0],[M,M]) # back to 2D image\n",
    "    y_2D_list.append(y_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_out_dist_plot(y_2D, weight_array, biases_array,nh,layer_size):\n",
    "    fig,ax=plt.subplots(ncols=3,nrows=1,figsize=(15,4))\n",
    "    fig.suptitle(\"Number of Hidden Layers: {a}     Hidden Layer Size: {b}\".format(a=nh,b=layer_size),c=(0.1, 0.2, 0.5),fontsize=16,fontweight='bold')    \n",
    "    \n",
    "    sns.distplot(weight_array, hist=True, ax=ax[0])\n",
    "    ax[0].set_xlabel(\"Weight\")\n",
    "    \n",
    "    sns.distplot(biases_array, hist=True, ax=ax[1],color='orange')\n",
    "    ax[1].set_xlabel(\"Bias\")\n",
    "    \n",
    "    img=ax[2].imshow(y_2D,origin='lower',extent=[-0.5,0.5,-0.5,0.5],interpolation='nearest')\n",
    "    ax[2].set_xlabel(r'$y_0$')\n",
    "    ax[2].set_ylabel(r'$y_1$')\n",
    "    ax[2].axes.xaxis.set_ticks([])\n",
    "    ax[2].axes.yaxis.set_ticks([])\n",
    "#     plt.show()\n",
    "\n",
    "i = 1\n",
    "filenames = [] \n",
    "for y_2D in y_2D_list:\n",
    "    filename = './fig{i}.jpg'.format(i=i)\n",
    "    filenames.append(filename)\n",
    "    y_out_dist_plot(y_2D=y_2D, weight_array=Weights, biases_array=Biases,nh=i,layer_size=LayerSize) \n",
    "    plt.savefig(filename,format='jpg',bbox_inches='tight',dpi=150)\n",
    "    i += 1\n",
    "    plt.clf()\n",
    "    \n",
    "\n",
    "# build gif\n",
    "with imageio.get_writer('./HW1_p3_Uniform_Nh{a}_LS{b}_high_w={c}_b={d}.gif'.format(a=Nh_max,b=LayerSize,c=high_w,d=high_b), mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "# Remove files\n",
    "for filename in set(filenames):\n",
    "    os.remove(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
